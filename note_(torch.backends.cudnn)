学习笔记：主要摘抄于https://zhuanlan.zhihu.com/p/73711222；括号{}中加入一些自己的理解，供自己学习记录。

  大家在训练深度学习模型的时候，经常会使用GPU来加速网络的训练。但是说起torch.backends.cudnn.benchmark这个GPU相关的flag，可能有人会感到比较陌生。
在一般场景下，只要简单地在pytorch程序开头将其值设置为True，就可以大大提升卷积神经网络的运行速度。既然如此神奇，为什么pytorch不将其默认设置为True？它
的适用场景是什么？为什么使用它可以提升效率？答案就在本文之中。

  
  浓缩版：
  设置torch.backends.cudnn.benchmark=True将会让程序在开始时花费一点额外时间，为整个网络的每个卷积层搜索最适合它的卷积实现算法，进而实现网络的加速。
适用场景是网络结构固定（不是动态变化的），网络的输入形状（包括batch size，图片大小，输入的通道）是不变的，其实也就是一般情况下都比较适用。反之，如果卷积
层的设置一直变化，将会导致程序不停地做优化，反而会耗费更多的时间。

  对于只想大致了解的读者，就此可以结束。
  
---------------------------------------------------------------------------------------------------------------------------------  
  详细版：
  
  背景知识：
  在说torch.backends.cudnn.benchmark之前，我们先简单介绍以下cuDNN。cuDNN是英伟达专门为深度神经网络开发出来的GPU加速库，针对卷积、池化等等常见操作
做了非常多的底层优化，比一般的GPU程序还要快很多。大多数主流深度学习框架都支持cuDNN，pytorch自然也不例外。在使用GPU的时候，pytorch会默认使用cuDNN加速。
但是，在使用cuDNN的时候，torch.backends.cudnn.benchmark模式是为False。所以就意味着，我们的程序也许还可以继续提速。
  {
  有过并行计算基础的同学们也许有印象，一款可以使用CUDA加速的算法，通常有无数种方式将其部署在CUDA上。我们可以根据算法的具体数学运算操作，参与数学运算的
对象尺寸来设置不同的thread grid结构，并应用不同的GPU内存。
  所以这里我猜测，简单的使用cuda对深度学习模型训练加速时，代码的底层只是使用了一种更generalized的方式将整个训练过程部署在了CUDA上。而
torch.backends.cudnn.benchmark则会针对此次训练任务使用到的网络结构（数学运算操作）、输入图像尺寸（参与运算的对象）尝试多种不同的部署方式，并benchmark
每一种部署方式的performance，最终选择最优解来部署此次网络训练过程）
  }

  卷积层是卷积神经网络中最重要的部分，也往往是运算量最大的部分。如果我们可以在底层代码中提升卷积运算的效率的话，就可以在不改变给定的神经网络结构的情况下，
大大提升其训练和预测的速度。
  对于卷积这个操作来说，其实现方式是多种多样的。最简单的实现方式就是使用多层循环嵌套，对于每张输入图像，对于每个要输出的通道，对于每个输入的通道，选取一个
区域，同指定卷积核进行卷积操作，然后逐行滑动，直到整张图像都处理完毕，这个方法一般被称为direct法，这个方法虽然简单，但是看到这么多循环，我们就知道效率
在一般情况不会很高了。除此之外，实现卷积层的算法还有基于GEMM（General Matrix Multiply）的，基于FFT的，基于Winograd算法的等等，而且每个算法还有自己
的一些变体。在一个开源的C++库triNNity中，就实现了接近80种的卷积向前传播算法！

  每种卷积算法，都有其特有的一些优势，比如有的算法在卷积核大的情况下，速度很快；有的算法在某些情况下内存使用比较小。给定一个卷积神经网络（比如ResNet-101），
给定输入图片的尺寸，给定硬件平台，实现这个网络最简单的方法就是对所有卷积层都采用相同的卷积算法（比如direct算法），但是这样运行肯定不是最优的；比较好的方法是，
我们可以预先进行一些简单的优化测试，在每一个卷积层中选择最适合（最快）的卷积算法，决定好每层最快的算法之后，我们再运行整个网络，这样效率就会提升不少。
  {
  为每一卷积层选择不同的卷积操作算法，不同的卷积操作算法在CUDA上的部署方式不同
  }
  
  为什么我们可以提前选择每层的卷积操作算法，即使每次我们送入网络训练的图片是不一样的？即使每次网络输入都是变化的，那么我怎么确保此次选出来的最优卷积操作组合
同样也适用于新的输入图像呢？原因是，对于给定输入来说，其具体的像素值大小不影响卷积操作的运行时间，只有其尺寸才会影响。也就是说，只要我们固定输入的尺寸为
（8，64，224，224），即batch size为8，输入通道为64，图像尺寸为224x224，那么卷积层的运行时间都是几乎不变的，无论每个具体像素值的大小是多少。
  这样的话，因为我们固定了模型输入的尺寸大小，所以对于每个卷积层来说，其接受的输入尺寸都是静态不变的。
  
  
  
  torch.backends.cudnn.benchmark
  说了这么多背景知识，但和cudnn.benchmark有什么联系呢？实际上，设置这个flag为True，我们就可以在pytorch中对模型里的卷积层进行预先的优化，也就是在每一个
卷积层中测试cuDNN提供的所有卷积实现算法，然后选择最快的那个。这样在模型启动的时候，只需要额外多花一点点预处理时间，就可以较大幅度地减少训练时间。

  这岂不是，用cudnn.benchmark一时爽，一直用一直爽吗？其实不然，在某些情况下，使用它可能会大大增加运行时间！在背景知识里我们提到过，但是在这里我们更加具体的
定义一下，到底哪些因素会影响到卷积层的运行时间。
  1.首先，当然是卷积层本身的参数，常见的包括卷积核大小，stride，dialation，padding，输出通道的个数等
  2.其次，是输入的相关参数，包括输入图片的宽和高，输入通道的个数，batch size等等
  3.最后，还有一些其他的因素，比如硬件平台（不同的并行计算算法在不同的GPU上表现也会有所不同）
  
  我们定义一个卷积场景的参数主要包括(1)和(2)，因为在同一个程序中(3)往往都是相同的，我们暂且忽略不计。不同的卷积场景有不同的最优卷积算法，需要分别进行测试和
选择。
  据此我们可以看出，首先如果我们的网络模型一直变的话，那肯定是不能设置cudnn.benchmark=True的。因为网络结构经常变，每次pytorch都会自动来根据新的卷积场景
来做优化：这次花费了很久的时间将模型训练算法的最优部署方式选出来，结果下次网络的结构变了，导致模型训练的数学运算发生变化，之前的最优部署失效。不仅如此，还得要
根据这个新的网络结构继续选择最优部署方式，又花费了很久的时间。
  另外，我们模型的输入大小也不能变。模型的输入就是数学操作的应用对象，对于一个卷积层，这次的输入形状如果是（8，3，224，224），下次变成了（8，3，112，112）。
模型的输入大小变化了，相当于整个模型训练算法发生了变化，那么之前的最优部署方式这时也许已经不是最优部署方式了，pytorch这时会重新寻找最优部署方式的。注意，这里的
batch size、输入通道，图片尺寸都不能改变。
  不过对于一般的CV模型来说，网络的结构一般不会动态发生变化的，其次，图像一般也都会resize到固定的尺寸，batch size也是固定的。所以，在大部分情况下，我们都
可以在程序中加上这行torch.backends.cudnn.benchmark=True，来减少总运行时间！



  那么这行代码要加到哪里
  之前在网上看到过一些博客提到使用cudnn.benchmark=True，但是没有明确说明这段代码要放在哪里。这里我怕有的读者和我当时一样，所以就加了这一小部分，其实一般
加在开头就好，比如在设置使用GPU的同时，后面补上一句：
  if args.use_gpu and torch.cuda.is_available():
    device = torch.device("cuda")
    torch.backends.cudnn.benchmark = True
  else:
    device = torch.device("cpu")
    
  当然在某些情况下也可以在程序中多次改变torch.backends.cudnn.benchmark的值
  

  
  
